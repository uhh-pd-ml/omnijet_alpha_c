# @package _global_

# to execute this experiment run:
# python gabbro/train.py experiment=example_experiment_tokenization

defaults:
  - override /data: data_tokenization_dev.yaml
  - override /model: model_vqvae_transformer.yaml
  - override /callbacks: tokenization_dummy_callbacks.yaml
  - override /trainer: ddp.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# add here checkpoint to continue training
#ckpt_path: "path/to/your/checkpoint.ckpt"

project_name: "tokenization_example"
tags: ["vqvae_tokenization"]

run_note: ""  # "here you can add a note to the run which will be displayed in the logging service"

seed: 1603
load_weights_from: false

data:
  data_dir: /data/dust/user/korcariw/maxwell.merged/CaloClouds/dataset/  # this is the path to the dataset
  batch_size: 16  # NOTE: adapt the limit_train_batches accordingly
  h5file: True
  dataset_kwargs_train:
    max_n_files_per_type: 1  # for this example we only load one file per type
    n_shower_per_file: 10000 # for this example we only load 10000 showers per file
    shuffle_only_once: true # shuffle the training dataset only once
  dataset_kwargs_val:
    shuffle_only_once: true  # shuffle the validation dataset only once
    n_shower_per_file: 1000  # for this example we only load 1000 showers per file
    seed_shuffle_data: 42
  dataset_kwargs_test:
    shuffle_only_once: true  # shuffle the test dataset only once
    n_shower_per_file: 1000 # for this example we only load 1000 showers per file
    seed_shuffle_data: 42
  dataset_kwargs_common:
    n_files_at_once: 1  # load 10 files at once (which are all in this case)
    load_only_once: true  # load the files only once and keep them in memory
    pad_length: 1700  # pad the showers to a length of 1700 hits
    energy_threshold: 0 # ignore hits with energy below this threshold
    energy_sorting: false  # sort the hits by energy (starting with the highest energy hit) (for the VQVAE this is irrelevant)
    feature_dict:
      x: {"multiply_by": 0.3, "subtract_by": 14.5}
      y: {"multiply_by": 0.3, "subtract_by": 14.5}
      z: {"multiply_by": 0.2, "subtract_by": 15.5}
      energy: {"multiply_by": 1, "subtract_by": -1,"func": "np.log","inv_func": "np.exp"}

trainer:
  max_epochs: 600
  gradient_clip_val: 1
  log_every_n_steps: 60
  limit_train_batches: 1.0  # 1.0 means all batches, 0.1 means 10% of all batches and e.g. 2700 means 2700 batches (to define the "epoch", which we might want to be smaller than the whole dataset to get faster feedback on the training process)
  limit_val_batches: 1.0  # --> using 200*512 = 102400 validation samples, around 10k per type

model:
  model_kwargs_loaded: null
  # --- optimizer configuration ---
  optimizer:
    _target_: gabbro.utils.optimizer.ranger.Ranger
    _partial_: true
    lr: 0.001
    weight_decay: 1e-2
    betas: [0.95, 0.999]
    eps: 1e-5
    alpha: 0.5
    k: 6

  # --- learning rate scheduler ---
  scheduler:
    _target_: gabbro.schedulers.lr_scheduler.OneCycleCooldown
    _partial_: true
    warmup: 10  # epochs until max_lr is reached
    cooldown: 20 # epochs to decrease to initial_lr after max_lr is reached
    cooldown_final: 50 # epochs to decrease to final_lr after max_lr is reached
    max_lr: 3e-4
    initial_lr: 3e-4
    final_lr: 3e-4  # final_lr is used after the second cooldown

  # --- model architecture configuration ---
  model_type: VQVAENormFormer
  model_kwargs:
    input_dim: 4
    hidden_dim: 128
    latent_dim: 12
    num_blocks: 4
    num_heads: 8
    alpha: 10
    vq_kwargs:
      num_codes: 65536 #32768
      beta: 0.9
      kmeans_init: false
      norm: null
      cb_norm: null
      affine_lr: 2
      sync_nu: 1
      replace_freq: 100

task_name: "tokenization"

logger:
  wandb:
    project: ${project_name}
    tags: ${tags}
    # group: ${project_name}
    name: ${task_name}
  comet:
    experiment_name: null
    project_name: ${project_name}
