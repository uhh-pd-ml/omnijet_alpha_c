{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import vector\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "sys.path.append(\"/data/dust/user/rosehenn/gabbro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shower generation with a trained OmniJet model\n",
    "\n",
    "This notebook provides a short example on how to load a trained OmniJet model with the next-token-prediction head and generate jets with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gabbro.models.backbone import BackboneNextTokenPredictionLightning\n",
    "\n",
    "# this checkpoint is the checkpoint from a backbone training with the nex-token-prediction head\n",
    "# make sure you have downloaded the checkpoint in advance\n",
    "# if not, run the script `checkpoints/download_checkpoints.sh`\n",
    "ckpt_path = \"/data/dust/user/rosehenn/gabbro_output/full_resolution/runs/2024-11-21_13-49-55_max-wng060_TerminativeCirculation/checkpoints/epoch_032_loss_4.10881.ckpt\"\n",
    "gen_model = BackboneNextTokenPredictionLightning.load_from_checkpoint(ckpt_path)\n",
    "gen_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Showers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_showers = gen_model.generate_n_showers_batched(\n",
    "    n_showers=2,\n",
    "    batch_size=2,\n",
    "    # saveas=save_path,  # use this option if you want to save the awkward array as a parquet file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_showers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the tokenizer model from checkpoint, and also get the feature_dict from the config ---\n",
    "from gabbro.models.vqvae import VQVAELightning\n",
    "\n",
    "ckpt_path = \"/data/dust/user/rosehenn/gabbro_output/TokTrain/runs/2024-09-21_16-54-39_max-wng062_CerousLocknut/checkpoints/epoch_231_loss_0.17179.ckpt\"\n",
    "\n",
    "vqvae_model = VQVAELightning.load_from_checkpoint(ckpt_path)\n",
    "vqvae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(Path(ckpt_path).parent.parent / \"config.yaml\")\n",
    "pp_dict = OmegaConf.to_container(cfg.data.dataset_kwargs_common.feature_dict)\n",
    "print(\"\\npp_dict:\")\n",
    "for item in pp_dict:\n",
    "    print(item, pp_dict[item])\n",
    "\n",
    "# get the cuts from the pp_dict (since this leads to particles being removed during\n",
    "# preprocessing/tokenization), thus we also have to remove them from the original jets\n",
    "# when we compare the tokenized+reconstructed particles to the original ones)\n",
    "pp_dict_cuts = {\n",
    "    feat_name: {\n",
    "        criterion: pp_dict[feat_name].get(criterion)\n",
    "        for criterion in [\"larger_than\", \"smaller_than\"]\n",
    "    }\n",
    "    for feat_name in pp_dict\n",
    "}\n",
    "\n",
    "print(\"\\npp_dict_cuts:\")\n",
    "for item in pp_dict_cuts:\n",
    "    print(item, pp_dict_cuts[item])\n",
    "\n",
    "print(\"\\nModel:\")\n",
    "print(vqvae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the generated tokens to physical features\n",
    "\n",
    "# note that if you want to reconstruct tokens from the generative model, you'll have\n",
    "# to remove the start token from the tokenized array, and subtract 1 from the tokens\n",
    "# (since we chose the convention to use 0 as the start token, so the tokens from the\n",
    "# generative model are shifted by 1 compared to the ones from the VQ-VAE)\n",
    "showers_reconstructed = vqvae_model.reconstruct_ak_tokens(\n",
    "    tokens_ak=generated_showers[:, 1:] - 1,\n",
    "    pp_dict=pp_dict,\n",
    "    batch_size=512,\n",
    "    pad_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showers_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gabbro.utils.mapping import merge_duplicates_numpy\n",
    "# merge duplicates in the reconstructed showers (since the VQ-VAE can produce duplicate tokens, which leads to duplicate particles after reconstruction)\n",
    "showers_reconstructed_merged = merge_duplicates_numpy(showers_reconstructed)\n",
    "showers_reconstructed_merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
